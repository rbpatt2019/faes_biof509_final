{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# FAES_BIOF509_FINAL\n",
    "\n",
    "Predicting neurodegeneration from global proteomics\n",
    "\n",
    "Project based on the [_drivendata/cookiecutter-data-science_](https://github.com/drivendata/cookiecutter-data-science) project structure\n",
    "\n",
    "[![Code Style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/ambv/black)\n",
    "[![GPL License](https://badges.frapsoft.com/os/gpl/gpl.svg?v=103)](https://opensource.org/licenses/GPL-3.0/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspiration\n",
    "\"Global quantitative analysis of the human brain proteome in Alzheimer’s and Parkinson’s Disease\"\n",
    "doi:10.1038/sdata.2018.36\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenges\n",
    "\n",
    "1. Dimensionality - 40 samples (10 per group) and ~12000 features\n",
    "1. Data Wrangling - the published data is not in particularly great in structure\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plan\n",
    "\n",
    "1. Data wrangling\n",
    "1. Exploratory data visualisation - volcano plot, tSNE\n",
    "1. Dimensionality reduction/feature selection\n",
    "1. Machine learning - comparison between classification and clustering\n",
    "1. Validation - Leave-one-out, as sample is so small\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "class CleanFrame(pd.core.frame.DataFrame):\n",
    "    \"\"\"Sub-classed DataFrame with expanded method for cleaning\n",
    "    \n",
    "    Frequently, when loading data, a number of cleaning steps are performed that do not have direct functions in the pandas module.\n",
    "    This class seeks to add those functionalities on top of pandas to expand its capacity\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    clean_cols: \n",
    "        Cleans column names by stripping white space, removing white space, and converting all characters to either lower or upper case\n",
    "    filter_by_val:\n",
    "        Select rows based on values in a given column\n",
    "    \"\"\"\n",
    "\n",
    "    @property\n",
    "    def _constructor(self):\n",
    "        return CleanFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_by_val(self, col=\"\", vals=[], keep=True, inplace=False):\n",
    "    # Type check inputs\n",
    "    for i in (keep, inplace):\n",
    "        if not isinstance(i, bool):\n",
    "            raise ValueError(f\"{i} must be a bool\")\n",
    "    if not isinstance(col, str):\n",
    "        raise ValueError(\"col must be a str in self.columns\")\n",
    "    if not isinstance(vals, (list, tuple)):\n",
    "        raise ValueError(\"vals must be a list or tuple\")\n",
    "\n",
    "    # Operate, checking whether to keep or discard\n",
    "    if keep:\n",
    "        new_data = self[self[col].isin(vals)]\n",
    "    else:\n",
    "        new_data = self[~self[col].isin(vals)]\n",
    "\n",
    "    # self._update_inplace is from pandas.core.frame\n",
    "    if inplace:\n",
    "        self._update_inplace(new_data)\n",
    "    else:\n",
    "        return new_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_data(\n",
    "    files, usecols=None, names=None, index_col=None, axis=0, join=\"outer\", keys=None\n",
    "):\n",
    "    # Type check files\n",
    "    if not isinstance(files, str):\n",
    "        raise ValueError(f\"files must be a str, not {type(files)}\")\n",
    "    # Find files\n",
    "    paths = glob.iglob(files)\n",
    "    # Read in files, sep=None with engine='python' will auto determine delim\n",
    "    reads = (\n",
    "        pd.read_csv(\n",
    "            file,\n",
    "            usecols=usecols,\n",
    "            header=0,\n",
    "            names=names,\n",
    "            index_col=index_col,\n",
    "            sep=None,\n",
    "            engine=\"python\",\n",
    "        )\n",
    "        for file in paths\n",
    "    )\n",
    "    # Convert to CleanFrame\n",
    "    cfs = (cf.CleanFrame(i) for i in reads)\n",
    "    # Clean data\n",
    "    clean = (i.prep_data() for i in cfs)\n",
    "    # Create final CleanFrame\n",
    "    data = cf.CleanFrame(\n",
    "        pd.concat(clean, axis=axis, join=join, keys=keys, sort=False, copy=False)\n",
    "    )\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Frontal cortex data\n",
    "    frontal = make_data(\n",
    "        \"data/raw/f*\",\n",
    "        usecols=[2, 5, 9, 10, 72, 73, 74, 75, 76, 77, 78, 79],\n",
    "        names=[\n",
    "            \"master\",\n",
    "            \"accession\",\n",
    "            \"q_score\",\n",
    "            \"pep_score\",\n",
    "            \"AD1\",\n",
    "            \"AD2\",\n",
    "            \"Control1\",\n",
    "            \"Control2\",\n",
    "            \"PD1\",\n",
    "            \"PD2\",\n",
    "            \"ADPD1\",\n",
    "            \"ADPD2\",\n",
    "        ],\n",
    "        index_col=1,\n",
    "        axis=1,\n",
    "        join=\"inner\",\n",
    "        keys=[1, 2, 3, 4, 5],\n",
    "    )\n",
    "    pd.to_pickle(frontal, \"data/interim/frontal_full.pkl\")\n",
    "#And again with the cingulate data...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![cingulate_volcano_plots](reports/figures/Anterior_Cingulate_Gyrus_Volcano_Plots.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![tsne frontal](reports/figures/tsne_frontal.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![tsne_cingulate](reports/figures/tsne_cingulate.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reducer for tSNE\n",
    "tsne_frontal = TSNEVisualizer(decompose=None, random_state=1, perplexity=10)\n",
    "tsne_frontal.fit(X_frontal, y_frontal)\n",
    "tsne_frontal.poof(outpath='reports/figures/tsne_frontal.png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![UMAP frontal](reports/figures/UMAP_Frontal.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![UMAP_cingulate](reports/figures/UMAP_Cingulate.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And That's it...so far!\n",
    "\n",
    "- Model selection wit yellowbricks classification report\n",
    "- Model tuning - gridsearchcv, but also yellowbrick\n",
    "   - Confusion matrix, ROCAUC, Precision-Recall, Validation Curve, Learning Curve\n",
    "- Refactoring!\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
